{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Depression detection (french).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1KhOC_vWASHN6naOXDOCVKfe_966NDPfT",
      "authorship_tag": "ABX9TyP16+zcEKpNXZIYCuptu2VJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AltafAllahAbbassi/Angular2020/blob/ex1/Depression_detection_(french).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRVAZX1cUr74"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "qLN3ItRTqAFg",
        "outputId": "5e628113-4401-4ccd-ac76-e6963106ee21"
      },
      "source": [
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/depression francais/Initial/fr1.csv\",encoding='utf-8', delimiter='\\t',names=[\"tweet\"])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1410922270424506373 2021-07-02 12:24:17 +0100 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1410907807352770560 2021-07-02 11:26:49 +0100 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1410904541294379011 2021-07-02 11:13:50 +0100 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1410884570493358080 2021-07-02 09:54:29 +0100 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1410861548797677568 2021-07-02 08:23:00 +0100 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet\n",
              "0  1410922270424506373 2021-07-02 12:24:17 +0100 ...\n",
              "1  1410907807352770560 2021-07-02 11:26:49 +0100 ...\n",
              "2  1410904541294379011 2021-07-02 11:13:50 +0100 ...\n",
              "3  1410884570493358080 2021-07-02 09:54:29 +0100 ...\n",
              "4  1410861548797677568 2021-07-02 08:23:00 +0100 ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "DxWGWEDoqaD2",
        "outputId": "63185346-b575-4c7b-a130-dbb00242b144"
      },
      "source": [
        "data=data[\"tweet\"]\n",
        "data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"1410922270424506373 2021-07-02 12:24:17 +0100 <cafard2luxe> Sinon manga y'a initial D et angel heart mais j'ai pas fini et niveau film j'ai mis que made in britain mais y'a aussi there will be blood, natural born killers, the big lebowski, the holy mountain, the deer hunter, possession, l'√©chelle de jacob, bay of blood  https://t.co/0YWKcQzhGQ\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46ykm5PGrHSZ"
      },
      "source": [
        "BAD_FORMAT_RE = re.compile('[0-9:+ -]*<[A-Za-z0-9_-]*>')\n",
        "c=\"1410922270424506373 2021-07-02 12:24:17 +0100 <cafard2luxe>  1410922270424506373 2021-07-02 12:24:17 +0100 <cafard2luxe> Sinon manga y'a initial D et angel heart mais j'ai pas fini et niveau film j'ai mis que made in britain mais y'a aussi there will be blood, natural born killers, the big lebowski, the holy mountain, the deer hunter, possession, l'√É¬©chelle de jacob, bay of blood  https://t.co/0YWKcQzhGQ\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaSKVIBGuJ34"
      },
      "source": [
        "formatted_tweets=[]\n",
        "for tweet in data:\n",
        "  tweet=BAD_FORMAT_RE.sub(' ', tweet,1)\n",
        "  tweet=tweet.strip()\n",
        "  formatted_tweets.append(tweet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAYI8EEdEJXO"
      },
      "source": [
        "initial_files=[\"fr1.csv\",\"fr2.csv\",\"fr3.csv\",\"fr4.csv\",\"fr5.csv\",\"fr6.csv\",\"fr7.csv\",\"fr8.csv\",\"fr9.csv\",\"fr10.csv\",\n",
        "               \"fr11.csv\",\"fr12.csv\",\"fr13.csv\",\"fr14.csv\",\"fr15.csv\",\"fr16.csv\",\"fr17.csv\",\"fr18.csv\",\"fr19.csv\",\"fr20.csv\",\n",
        "               \"fr21.csv\",\"fr22.csv\",\"fr23.csv\",\"fr24.csv\",\"fr25.csv\",\"fr26.csv\",\"fr27.csv\"]\n",
        "\n",
        "\n",
        "form_files=[\"fr1_\",\"fr2_\",\"fr3_\",\"fr4_\",\"fr5_\",\"fr6_\",\"fr7_\",\"fr8_\",\"fr9_\",\"fr10_\",\n",
        "               \"fr11_\",\"fr12_\",\"fr13_\",\"fr14_\",\"fr15_\",\"fr16_\",\"fr17_\",\"fr18_\",\"fr19_\",\"fr20_\",\n",
        "               \"fr21_\",\"fr22_\",\"fr23_\",\"fr24_\",\"fr25_\",\"fr26_\",\"fr27_\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M_SPyxZvFy5"
      },
      "source": [
        "import csv \n",
        "with open('fr1_.csv', mode='w',encoding='utf8') as file:\n",
        "    employee_writer = csv.writer(file, delimiter='\\t')\n",
        "    for t in formatted_tweets:\n",
        "      employee_writer.writerow([t])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dT2YsyT8ldw"
      },
      "source": [
        "# data = pd.read_csv(\"/content/drive/MyDrive/depression francais/Initial/fr17.csv\",encoding='utf-8', engine='python',delimiter='\\t',names=[\"tweet\"])\n",
        "# data.head()\n",
        "# data=data[\"tweet\"]\n",
        "# formatted_tweets=[]\n",
        "# for tweet in data:\n",
        "#   tweet=BAD_FORMAT_RE.sub(' ', tweet,1)\n",
        "#   tweet=tweet.strip()\n",
        "#   formatted_tweets.append(tweet)\n",
        "\n",
        "# with open('fr17_.csv'+'_', mode='w',encoding='utf8') as file:\n",
        "#     employee_writer = csv.writer(file, delimiter='\\t')\n",
        "#     for t in formatted_tweets:\n",
        "#       employee_writer.writerow([t])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRnNishjFZ8a"
      },
      "source": [
        "def format_file(file_name):\n",
        "  data = pd.read_csv(\"/content/drive/MyDrive/depression francais/Initial/\"+file_name,encoding='utf-8', engine='python',delimiter='\\t',names=[\"tweet\"])\n",
        "  data.head()\n",
        "  data=data[\"tweet\"]\n",
        "  formatted_tweets=[]\n",
        "  for tweet in data:\n",
        "    tweet=BAD_FORMAT_RE.sub(' ', tweet,1)\n",
        "    tweet=tweet.strip()\n",
        "    formatted_tweets.append(tweet)\n",
        "\n",
        "  with open(file_name, mode='w',encoding='utf8') as file:\n",
        "    employee_writer = csv.writer(file, delimiter='\\t')\n",
        "    for t in formatted_tweets:\n",
        "      employee_writer.writerow([t])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mKJkg1NGrF-"
      },
      "source": [
        "for file in initial_files:\n",
        "  format_file(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf0OslHSHRfS"
      },
      "source": [
        "Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03mcW5zbHUCT",
        "outputId": "095668c2-3625-4d16-e359-7ac367bac01c"
      },
      "source": [
        "!pip install itranslate -U\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting itranslate\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/63/0b2fa2bfadb8e153f141c06c0b97476e7daa10459044bbd1c30b55d11b3c/itranslate-0.1.1-py3-none-any.whl\n",
            "Collecting logzero<2.0.0,>=1.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/68/aa714515d65090fcbcc9a1f3debd5a644b14aad11e59238f42f00bd4b298/logzero-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: joblib<2.0.0,>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from itranslate) (1.0.1)\n",
            "Collecting httpx<0.19.0,>=0.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/dc/fea40314b49f041fb9e957977f73bc7a9e07e67b8e3cb06eacede32576ec/httpx-0.18.2-py3-none-any.whl (76kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 4.0MB/s \n",
            "\u001b[?25hCollecting httpcore<0.14.0,>=0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/94/8c136dbfac58456395dab9eb71e156cd14e0449f013f6c9c007e3ef4a160/httpcore-0.13.6-py3-none-any.whl (58kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.7/dist-packages (from httpx<0.19.0,>=0.18.2->itranslate) (2021.5.30)\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl\n",
            "Collecting h11<0.13,>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/0f/7a0eeea938eaf61074f29fed9717f2010e8d0e0905d36b38d3275a1e4622/h11-0.12.0-py3-none-any.whl (54kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 6.3MB/s \n",
            "\u001b[?25hCollecting anyio==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/8c/6712b0aebe9b250736ec5dde99883b143290b49ecc2310eb583577e316aa/anyio-3.2.1-py3-none-any.whl (75kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: idna; extra == \"idna2008\" in /usr/local/lib/python3.7/dist-packages (from rfc3986[idna2008]<2,>=1.3->httpx<0.19.0,>=0.18.2->itranslate) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from anyio==3.*->httpcore<0.14.0,>=0.13.3->httpx<0.19.0,>=0.18.2->itranslate) (3.7.4.3)\n",
            "Installing collected packages: logzero, h11, sniffio, anyio, httpcore, rfc3986, httpx, itranslate\n",
            "Successfully installed anyio-3.2.1 h11-0.12.0 httpcore-0.13.6 httpx-0.18.2 itranslate-0.1.1 logzero-1.7.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ox50qcxcJRwV",
        "outputId": "a6dd8dbe-60c4-40f6-c6fd-1e1d9968ff84"
      },
      "source": [
        "from itranslate import itranslate as itrans\n",
        "\n",
        "\n",
        "itrans(\"ÊπØËàπ„Å´Êµ∏„Åã„Å£„Å¶„Åü„ÇâÁõÆ„ÅÆÂâç„Å´„Éá„ÉÉ„Ç´„Ç§GÈôçËá®„ÄÇ „Éê„Çπ„Çø„Ç™„É´„Å†„ÅëÂºï„Å£Êé¥„Çì„Å†„Å≥„Å°„Çá„Å≥„Å°„ÇáÂçäË£∏„ÅßÂ§´„ÇíÂëº„Å≥„ÄÅÈÄÄÊ≤ª„Åó„Å¶„ÇÇ„Çâ„ÅÑ„Åæ„Åó„Åü‚Ä¶ „Åì„Çì„Å™ÊôÇ„Å´„Å¥„Å£„Åü„Çä„Å™Ë°®Áèæ„ÄÅavoir le cafardü™≥ cafard„ÅØG„ÅÆ„Åì„Å®„ÄÇ G„ÇíÊåÅ„Å£„Å¶„Çã„ÄÅ„Åß ÊÇ≤„Åó„ÅÑ„ÄÅËêΩ„Å°Ëæº„Çì„Åß„Çã„ÄÅ„Å®„ÅÑ„ÅÜÊÑèÂë≥„Åß„Åô„ÄÇ ‰ªä„ÅÆÁßÅ„ÅØJ'ai le cafardüò¢ #„Éï„É©„É≥„ÇπË™û\", to_lang=\"fr\")  # 'Testen Sie das und das'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Dekkai G aventure devant vous s'il √©tait immerg√© dans une baignoire. Seule une serviette de bain tir√©e et appel√©e son mari et s'est d√©barrass√© de son mari ... une expression aussi opportune, avoir le Cafardu0001fab3 Cafard est G. Cela signifie que j'ai g, triste, d√©prim√©. Maintenant je suis J'ai Le Cafardüò¢ # Fran√ßais\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r1GJgoCMxDh"
      },
      "source": [
        "import time\n",
        "def translate(text):\n",
        "  return itrans(text, to_lang=\"fr\")\n",
        "\n",
        "\n",
        "def translate_file(file_name):\n",
        "  data = pd.read_csv(\"/content/drive/MyDrive/depression francais/formatted/\"+file_name+\".csv\",encoding='utf-8', engine='python',delimiter='\\t',names=[\"tweet\"])\n",
        "  # data.head()\n",
        "  data=data[\"tweet\"]\n",
        "  tran_tweets=[]\n",
        "  for tweet in data:\n",
        "    time.sleep(0.5)\n",
        "    tweet=translate(tweet)\n",
        "    tran_tweets.append(tweet)\n",
        "\n",
        "\n",
        "  with open(file_name+\"tran.csv\", mode='w',encoding='utf8') as file:\n",
        "    writer = csv.writer(file, delimiter='\\t')\n",
        "    for t in tran_tweets:\n",
        "      writer.writerow([t])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMQU63ZnysiU"
      },
      "source": [
        "for file in initial_files:\n",
        "  translate_file(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgimVoEfd2qV"
      },
      "source": [
        "Concatinating texts and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LFN94N_d1mO"
      },
      "source": [
        "depressive_data=[]\n",
        "for file in form_files:\n",
        "  data = pd.read_csv(\"/content/drive/MyDrive/depression francais/formatted/\"+file+\".csv\",encoding='utf-8', engine='python',delimiter='\\t',names=[\"tweet\"])\n",
        "  tweets=data[\"tweet\"]\n",
        "  for t in tweets:\n",
        "    depressive_data.append(t)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYNEHHEEilPN",
        "outputId": "f7e1380f-6752-44a3-ee13-7376a7cab7f9"
      },
      "source": [
        "print(\"Depressive tweets\",len(depressive_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Depressive tweets 59958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_LqWGQ8iruZ"
      },
      "source": [
        "dataset_dep = pd.DataFrame({'tweet': depressive_data, 'label': 1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIAvOEcgpHwz"
      },
      "source": [
        "twitter_data = pd.read_csv('/content/drive/MyDrive/depression francais/french_tweets.csv', encoding='utf-8',delimiter=',' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "F1FMUv8AxIyF",
        "outputId": "d23265ec-c490-4857-a3a9-80bbd6bc4e3e"
      },
      "source": [
        "twitter_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>- Awww, c'est un bummer. Tu devrais avoir davi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Est contrari√© qu'il ne puisse pas mettre √† jou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>J'ai plong√© plusieurs fois pour la balle. A r√©...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Tout mon corps a des d√©mangeaisons et comme si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Non, il ne se comporte pas du tout. je suis en...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      0  - Awww, c'est un bummer. Tu devrais avoir davi...\n",
              "1      0  Est contrari√© qu'il ne puisse pas mettre √† jou...\n",
              "2      0  J'ai plong√© plusieurs fois pour la balle. A r√©...\n",
              "3      0  Tout mon corps a des d√©mangeaisons et comme si...\n",
              "4      0  Non, il ne se comporte pas du tout. je suis en..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AZFkyDMyhQl"
      },
      "source": [
        " twitter_data=twitter_data.copy().sample(60000, random_state=42) #60 000 random tweets(classified as non-depressed) ,so that our dataset will be balanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpIiFzW-m6ct",
        "outputId": "454f6b20-0a08-4ef2-ebf1-6e2d0d8292a4"
      },
      "source": [
        "len(twitter_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "_xVD-AwC7vzF",
        "outputId": "250f30ec-e24f-44b0-a860-7694fb313885"
      },
      "source": [
        "twitter_data.rename(columns=\n",
        "{\n",
        "\"text\": \"tweet\"\n",
        "}, inplace=True)\n",
        "twitter_data[\"label\"]=0\n",
        "dataset_non_dep=twitter_data\n",
        "print(len(dataset_non_dep))\n",
        "dataset_non_dep.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>471040</th>\n",
              "      <td>0</td>\n",
              "      <td>El som et la soci√©t√© humaine avec cindy! Je ve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1340507</th>\n",
              "      <td>0</td>\n",
              "      <td>Quel beau jour d'√©t√©</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>757669</th>\n",
              "      <td>0</td>\n",
              "      <td>D√©teste assis √† l'int√©rieur quand c'est une be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119639</th>\n",
              "      <td>0</td>\n",
              "      <td>Lundi matin m√™me pas le soleil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1190949</th>\n",
              "      <td>0</td>\n",
              "      <td>La soeur a juste √©t√© bronz√©e: soph: 'what are ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         label                                              tweet\n",
              "471040       0  El som et la soci√©t√© humaine avec cindy! Je ve...\n",
              "1340507      0                               Quel beau jour d'√©t√©\n",
              "757669       0  D√©teste assis √† l'int√©rieur quand c'est une be...\n",
              "119639       0                     Lundi matin m√™me pas le soleil\n",
              "1190949      0  La soeur a juste √©t√© bronz√©e: soph: 'what are ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "o3U2xUC_87bG",
        "outputId": "0ff8759f-0842-4954-d1ec-235f290d6280"
      },
      "source": [
        "dataset = pd.concat([dataset_dep,dataset_non_dep]) # merge the dataset on normal tweets and depressive tweets\n",
        "dataset = dataset.sample(frac=1)  # shuffle the dataset\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14288</th>\n",
              "      <td>@_kaedeazusagawa J'suis abattu</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14215</th>\n",
              "      <td>@Foxdere_ @CestTakumi j'suis abattu j'√©tais cr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>582091</th>\n",
              "      <td>Non, ne pouvais pas le balancer cette ann√©e. M...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481467</th>\n",
              "      <td>d√©but de nuit. Je dois me lever √† 7 heures du ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5598</th>\n",
              "      <td>Dernier contr√¥le post op, c est tout bon. Je p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    tweet  label\n",
              "14288                      @_kaedeazusagawa J'suis abattu      1\n",
              "14215   @Foxdere_ @CestTakumi j'suis abattu j'√©tais cr...      1\n",
              "582091  Non, ne pouvais pas le balancer cette ann√©e. M...      0\n",
              "481467  d√©but de nuit. Je dois me lever √† 7 heures du ...      0\n",
              "5598    Dernier contr√¥le post op, c est tout bon. Je p...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odiHrfz2pm4_"
      },
      "source": [
        "Cleaning:\n",
        "\n",
        "* remove identification (@sign ) & hashtags\n",
        "* remove URLS\n",
        "*   Replace emojis with text : use emojis library https://pypi.org/project/emoji/ <br> <bold>search for dictionnary alternative</bold>\n",
        "*   If I found a contraction file , replace contractions\n",
        "*   Remove stop words : https://github.com/stopwords-iso/stopwords-fr , or from nltk \n",
        "*   Remove punctuations, numbers\n",
        "*   Remove non-latin letters\n",
        "*   Lower case \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2vX9gf6F6iq",
        "outputId": "325d33a1-b0c1-4fb4-8236-4845c03666da"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_944DHhQjE4"
      },
      "source": [
        "#construct an emoji dictionnary\n",
        "emojis = {}\n",
        "emojis_df = pd.read_csv('/content/drive/MyDrive/depression francais/emojis2fr.csv', encoding='utf-8',delimiter=',' )\n",
        "emoji_df = emojis_df[\"emoji\"]\n",
        "text_df = emojis_df[\"text\"]\n",
        "for i in range(len(emojis_df)):\n",
        "  emojis[emoji_df[i]]=text_df[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9pRqz_VDvy2"
      },
      "source": [
        "import string\n",
        "import regex\n",
        "IDENT_FORMAT_RE = re.compile('@[0-9a_zA-z_]*')\n",
        "NUMBER_FORMAT_RE = re.compile('[0-9]')\n",
        "HASHTAG_FORMAT_RE = re.compile('#[0-9a_zA-z_]*')\n",
        "FRENCH_STOP_WORDS = stopwords.words('french') \n",
        "\n",
        "\n",
        "\n",
        "def remove_iden(text):\n",
        "  text=IDENT_FORMAT_RE.sub('', text)\n",
        "  text=text.strip()\n",
        "  return text\n",
        "\n",
        "def remove_multiple_spaces(text):\n",
        "  return re.sub('\\s+',' ',text)\n",
        "\n",
        "def remove_punctuations(text):\n",
        "  text=' '.join(word.strip( string.punctuation) for word in text.split())\n",
        "  text=text.strip()\n",
        "  return text\n",
        "\n",
        "def remove_numbers(text):#remove numbers after ident !!!\n",
        "  text = NUMBER_FORMAT_RE.sub('', text)\n",
        "  return text\n",
        "\n",
        "\n",
        "def remove_urls(text):\n",
        "  text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
        "  return text\n",
        "\n",
        "def remove_hashtags(text):\n",
        "  text = HASHTAG_FORMAT_RE.sub('', text)\n",
        "  text = text.strip()\n",
        "  return text\n",
        "\n",
        "def remove_stop_words(text):\n",
        "  words=word_tokenize(text)\n",
        "  text=' '.join(word for word in words if word not in FRENCH_STOP_WORDS )\n",
        "  return text\n",
        "\n",
        "\n",
        "def tranform_emojis(text):\n",
        "  words=word_tokenize(text)\n",
        "  for i in range(len(words)):\n",
        "    if words[i] in emojis:\n",
        "      words[i] = emojis[words[i]]\n",
        "  text=' '.join(word for word in words )\n",
        "  return text\n",
        "\n",
        "def keep_only_latin(text):\n",
        "  text = regex.sub(\"[^\\p{Latin}'-]\", ' ', text)\n",
        "  # text = LATIN_FORMAT_RE.sub(\" \", text)\n",
        "  return text\n",
        "\n",
        "def clean(text):\n",
        "  text = remove_iden(text)\n",
        "  text = remove_urls(text)\n",
        "  text = tranform_emojis(text)\n",
        "  text = remove_hashtags(text)\n",
        "  text = remove_stop_words(text)\n",
        "  text = remove_punctuations(text)\n",
        "  text = remove_numbers(text)\n",
        "  text = keep_only_latin(text)\n",
        "  text = text.lower()\n",
        "  text = remove_multiple_spaces(text)\n",
        "  text = text.strip()\n",
        "  return text\n",
        "\n",
        "# print(tranform_emojis(\"@AfMouke Alors malheureusement je n'ai que ce screen pour ma d√©fense üòî mais pour la 2e phase de isshin le moment o√π il saute vers l'avant avec sa lance j'ai beau esquiver super bien et j'ai un t√©moin bah je prend full d√©g√¢t üòê  https://t.co/tzig1cS2SQ\"))\n",
        "# print(remove_stop_words(\"@AfMouke Alors malheureusement je n'ai que ce screen pour ma d√©fense üòî mais pour la 2e phase de isshin le moment o√π il saute vers l'avant avec sa lance j'ai beau esquiver super bien et j'ai un t√©moin bah je prend full d√©g√¢t üòê  https://t.co/tzig1cS2SQ\"))\n",
        "# print(remove_urls(\"@AfMouke Alors malheureusement je n'ai que ce screen pour ma d√©fense üòî mais pour la 2e phase de isshin le moment o√π il saute vers l'avant avec sa lance j'ai beau esquiver super bien et j'ai un t√©moin bah je prend full d√©g√¢t üòê  https://t.co/tzig1cS2SQ\"))\n",
        "# print(remove_punctuations(\"@AfMouke Alors malheureusement-cc . , je n'ai que ce screen pour ma d√©fense üòî mais pour la 2e phase de isshin le moment o√π il saute vers l'avant avec sa lance j'ai beau esquiver super bien et j'ai un t√©moin bah je prend full d√©g√¢t üòê  https://t.co/tzig1cS2SQ\"))\n",
        "\n",
        "# print(clean(\"@AfMouke Alors malheureusement-cc  ŸÑŸÑŸÑŸÑ. , je n'ai que ce screen pour ma d√©fense üòî mais pour la 2e phase de isshin  le moment o√π il saute vers l'avant avec sa lance j'ai beau esquiver super bien et j'ai un t√©moin bah je prend full d√©g√¢t üòê  https://t.co/tzig1cS2SQ\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjI1HIWASPoE"
      },
      "source": [
        "tweets=list(dataset[\"tweet\"])\n",
        "Y=list(dataset[\"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9c7Vh3OX1HF",
        "outputId": "5ca3504f-6cd1-4f84-d209-b6d284dbccb2"
      },
      "source": [
        "print(len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "119958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_VMpZXsS6D7"
      },
      "source": [
        "X=[]\n",
        "for t in tweets:\n",
        "  try:\n",
        "    x=clean(str(t))\n",
        "    X.append(x)\n",
        "  except :\n",
        "    print(\"t\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJUObPEeXwSp",
        "outputId": "e741caea-bfd2-4ddc-ad7a-69becd9a3c1f"
      },
      "source": [
        "print(len(X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "119958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnQJmdXLYZ3V",
        "outputId": "8f9ffe0a-c109-4a08-a6d5-f2360bfbfd30"
      },
      "source": [
        "print(X[1])\n",
        "print(tweets[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "j'suis abattu j'√©tais crev√© c'est horrible gros\n",
            "@Foxdere_ @CestTakumi j'suis abattu j'√©tais crev√© en + c'est horrible gros\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G2EOUyeaCxF"
      },
      "source": [
        "Modeling :\n",
        "Spliting the dataset :80 % 20 %\n",
        "* Naive Bayes = 0.95 \n",
        "* Logistic Regression = 0.96\n",
        "* Linear Support Vector = 0.92\n",
        "* SVC = 0.98"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf_co3LpbDyL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OfzvZEMcSbS"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1AAO6DLcbvy"
      },
      "source": [
        "**Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv1I-8mycfvn",
        "outputId": "2110b882-22b4-4452-a159-9807cc7d4fd8"
      },
      "source": [
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "nb.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OTiKp6cc4Jh",
        "outputId": "94037b70-6e37-4d7e-e591-1e2d9fe82c08"
      },
      "source": [
        "y_pred = nb.predict(x_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred, digits=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.9547349116372124\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.97890   0.93007   0.95386     12070\n",
            "           1    0.93261   0.97970   0.95558     11922\n",
            "\n",
            "    accuracy                        0.95473     23992\n",
            "   macro avg    0.95575   0.95489   0.95472     23992\n",
            "weighted avg    0.95590   0.95473   0.95471     23992\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9O20wgwdXmU"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ1SXabudXOH",
        "outputId": "cace3a48-4cff-4a90-9327-7bead41ae7cd"
      },
      "source": [
        "logreg = Pipeline([('vect',CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
        "               ])\n",
        "logreg.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=1, penalty='l2',\n",
              "                                    random_state=None, solver='lbfgs',\n",
              "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_re5XPOeF3r",
        "outputId": "2ce165a2-f0f0-4066-f831-356d8702e0dd"
      },
      "source": [
        "y_pred = logreg.predict(x_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred, digits=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.9630293431143715\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.97317   0.95278   0.96287     12070\n",
            "           1    0.95318   0.97341   0.96319     11922\n",
            "\n",
            "    accuracy                        0.96303     23992\n",
            "   macro avg    0.96318   0.96309   0.96303     23992\n",
            "weighted avg    0.96324   0.96303   0.96303     23992\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U319fWx_f_bL"
      },
      "source": [
        "**Linear Support Vector**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etkm9eMqel9m",
        "outputId": "6b8e3a1c-132f-4440-ce5a-0ca548b69ed5"
      },
      "source": [
        "sgd = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
        "               ])\n",
        "sgd.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=Non...\n",
              "                ('clf',\n",
              "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
              "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
              "                               fit_intercept=True, l1_ratio=0.15,\n",
              "                               learning_rate='optimal', loss='hinge',\n",
              "                               max_iter=5, n_iter_no_change=5, n_jobs=None,\n",
              "                               penalty='l2', power_t=0.5, random_state=42,\n",
              "                               shuffle=True, tol=None, validation_fraction=0.1,\n",
              "                               verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1xMnTuwgD09",
        "outputId": "f3d7b446-af1d-43ba-e0e1-941da97c11b9"
      },
      "source": [
        "y_pred = sgd.predict(x_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred, digits=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.9212237412470824\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.87581   0.98277   0.92621     12070\n",
            "           1    0.98009   0.85892   0.91551     11922\n",
            "\n",
            "    accuracy                        0.92122     23992\n",
            "   macro avg    0.92795   0.92084   0.92086     23992\n",
            "weighted avg    0.92763   0.92122   0.92090     23992\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTWJlrnAgWNh"
      },
      "source": [
        "**SVC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgOYwFUCgUOy",
        "outputId": "a940cfc2-7611-4778-80fc-9e0737aa51a0"
      },
      "source": [
        "svc = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', SVC()),\n",
        "              ])\n",
        "svc.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySeSBId9grLv",
        "outputId": "0041f4a5-bc93-4493-a5a9-40fbc61b7e3d"
      },
      "source": [
        "y_pred = svc.predict(x_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred, digits=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.9817855951983995\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.97346   0.99080   0.98206     12070\n",
            "           1    0.99052   0.97266   0.98151     11922\n",
            "\n",
            "    accuracy                        0.98179     23992\n",
            "   macro avg    0.98199   0.98173   0.98178     23992\n",
            "weighted avg    0.98194   0.98179   0.98178     23992\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}